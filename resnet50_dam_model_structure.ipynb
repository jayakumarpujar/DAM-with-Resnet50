{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b46555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 12:38:26.357197: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-16 12:38:26.397766: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-16 12:38:26.398250: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-16 12:38:27.304349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import cv2,os\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D , BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f6df19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 4 classes.\n",
      "Found 400 images belonging to 4 classes.\n",
      "Found 400 images belonging to 4 classes.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 12:42:58.851015: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - ETA: 0s - loss: 1.2989 - accuracy: 0.4200"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 12:44:22.287679: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 110s 3s/step - loss: 1.2989 - accuracy: 0.4200 - val_loss: 1.3297 - val_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "38/38 [==============================] - 104s 3s/step - loss: 0.9682 - accuracy: 0.4908 - val_loss: 0.8916 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "38/38 [==============================] - 116s 3s/step - loss: 0.7999 - accuracy: 0.5000 - val_loss: 1.8479 - val_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "38/38 [==============================] - 115s 3s/step - loss: 0.7546 - accuracy: 0.5667 - val_loss: 0.7623 - val_accuracy: 0.4875 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "38/38 [==============================] - 110s 3s/step - loss: 0.7024 - accuracy: 0.6317 - val_loss: 0.7105 - val_accuracy: 0.7150 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "38/38 [==============================] - 103s 3s/step - loss: 0.6658 - accuracy: 0.6975 - val_loss: 0.6582 - val_accuracy: 0.7475 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "38/38 [==============================] - 109s 3s/step - loss: 0.6243 - accuracy: 0.7500 - val_loss: 0.6567 - val_accuracy: 0.6525 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "38/38 [==============================] - 109s 3s/step - loss: 0.5592 - accuracy: 0.7808 - val_loss: 0.5530 - val_accuracy: 0.7750 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "38/38 [==============================] - 109s 3s/step - loss: 0.4995 - accuracy: 0.8083 - val_loss: 0.4515 - val_accuracy: 0.8250 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "38/38 [==============================] - 109s 3s/step - loss: 0.4715 - accuracy: 0.8258 - val_loss: 0.4144 - val_accuracy: 0.8475 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "38/38 [==============================] - 110s 3s/step - loss: 0.4464 - accuracy: 0.8283 - val_loss: 0.4042 - val_accuracy: 0.8450 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "38/38 [==============================] - 109s 3s/step - loss: 0.4235 - accuracy: 0.8358 - val_loss: 0.4425 - val_accuracy: 0.7950 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "38/38 [==============================] - 109s 3s/step - loss: 0.3752 - accuracy: 0.8683 - val_loss: 0.4235 - val_accuracy: 0.8375 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "38/38 [==============================] - 109s 3s/step - loss: 0.3888 - accuracy: 0.8525 - val_loss: 0.3796 - val_accuracy: 0.8600 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "38/38 [==============================] - 109s 3s/step - loss: 0.3597 - accuracy: 0.8700 - val_loss: 0.4502 - val_accuracy: 0.8225 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "38/38 [==============================] - 109s 3s/step - loss: 0.4012 - accuracy: 0.8458 - val_loss: 0.3841 - val_accuracy: 0.8525 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "38/38 [==============================] - 109s 3s/step - loss: 0.3421 - accuracy: 0.8692 - val_loss: 0.3587 - val_accuracy: 0.8400 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "38/38 [==============================] - 109s 3s/step - loss: 0.3485 - accuracy: 0.8717 - val_loss: 0.3403 - val_accuracy: 0.8525 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "38/38 [==============================] - 109s 3s/step - loss: 0.3397 - accuracy: 0.8708 - val_loss: 0.3103 - val_accuracy: 0.8950 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "38/38 [==============================] - 109s 3s/step - loss: 0.3127 - accuracy: 0.8933 - val_loss: 0.3369 - val_accuracy: 0.8650 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "38/38 [==============================] - 109s 3s/step - loss: 0.2863 - accuracy: 0.8892 - val_loss: 0.2470 - val_accuracy: 0.9125 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "38/38 [==============================] - 110s 3s/step - loss: 0.3049 - accuracy: 0.8867 - val_loss: 0.2822 - val_accuracy: 0.8850 - lr: 0.0010\n",
      "Epoch 23/30\n",
      "38/38 [==============================] - 109s 3s/step - loss: 0.2790 - accuracy: 0.8933 - val_loss: 0.2062 - val_accuracy: 0.9325 - lr: 0.0010\n",
      "Epoch 24/30\n",
      "38/38 [==============================] - 110s 3s/step - loss: 0.2988 - accuracy: 0.8925 - val_loss: 0.3069 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 25/30\n",
      "38/38 [==============================] - 116s 3s/step - loss: 0.2619 - accuracy: 0.9050 - val_loss: 0.2161 - val_accuracy: 0.9325 - lr: 0.0010\n",
      "Epoch 26/30\n",
      "38/38 [==============================] - 115s 3s/step - loss: 0.2700 - accuracy: 0.9025 - val_loss: 0.2266 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Epoch 27/30\n",
      "38/38 [==============================] - 119s 3s/step - loss: 0.2855 - accuracy: 0.8975 - val_loss: 0.2167 - val_accuracy: 0.9250 - lr: 0.0010\n",
      "Epoch 28/30\n",
      "38/38 [==============================] - 114s 3s/step - loss: 0.2558 - accuracy: 0.9017 - val_loss: 0.2509 - val_accuracy: 0.9100 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 13:35:00.087673: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 28s 2s/step - loss: 0.2509 - accuracy: 0.9100\n",
      "Test Accuracy: 0.9100000262260437\n"
     ]
    }
   ],
   "source": [
    "# Define the DAM layer\n",
    "class DenseAssociativeMemoryLayer(layers.Layer):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(DenseAssociativeMemoryLayer, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                      shape=(input_shape[-1], self.num_classes),\n",
    "                                      initializer=\"random_normal\",\n",
    "                                      trainable=True)\n",
    "        super(DenseAssociativeMemoryLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # DAM operation as described in the paper\n",
    "        inputs_normalized = tf.math.l2_normalize(inputs, axis=-1)\n",
    "        kernel_normalized = tf.math.l2_normalize(self.kernel, axis=0)\n",
    "        output = tf.matmul(inputs_normalized, kernel_normalized)\n",
    "        output = tf.nn.softmax(output)\n",
    "        return output\n",
    "\n",
    "num_classes = 4\n",
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "x = resnet_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = DenseAssociativeMemoryLayer(num_classes=num_classes)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs = resnet_model.input, outputs = output)\n",
    "\n",
    "for layer in resnet_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Data generators\n",
    "train_gen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rescale=1/255.)\n",
    "val_gen = ImageDataGenerator(rescale=1/255.)\n",
    "test_gen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "train_dir = '/home/jayakumar/Documents/Re_ Resume/coffee_data/train'\n",
    "val_dir = '/home/jayakumar/Documents/Re_ Resume/coffee_data/valid'\n",
    "test_dir = '/home/jayakumar/Documents/Re_ Resume/coffee_data/test'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SEED = 56\n",
    "CLASS_MODE = 'categorical'\n",
    "channels = 3\n",
    "img_size = (224, 224)\n",
    "\n",
    "train_flow_gen = train_gen.flow_from_directory(directory=train_dir,\n",
    "                                              class_mode= CLASS_MODE,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              target_size=img_size,\n",
    "                                              seed=SEED)\n",
    "\n",
    "val_flow_gen = val_gen.flow_from_directory(directory=val_dir,\n",
    "                                            class_mode=CLASS_MODE,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            target_size=img_size,\n",
    "                                            seed=SEED)\n",
    "\n",
    "test_flow_gen = test_gen.flow_from_directory(directory=test_dir,\n",
    "                                            class_mode=CLASS_MODE,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            target_size=img_size,\n",
    "                                            seed=SEED)\n",
    "\n",
    "# Compile the model\n",
    "optimiser = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "rlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=6, mode='min', verbose=0)\n",
    "early_cb = EarlyStopping(monitor='val_loss', patience=5, mode='min', verbose=0)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_flow_gen, epochs=30,\n",
    "          steps_per_epoch=int(np.ceil(train_flow_gen.samples / BATCH_SIZE)),\n",
    "          validation_data=val_flow_gen,\n",
    "          validation_steps=int(np.ceil(val_flow_gen.samples / BATCH_SIZE)),\n",
    "          callbacks=[rlr_cb, early_cb])\n",
    "\n",
    "# model.save(\"res_model.h5\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_flow_gen)\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57572187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
